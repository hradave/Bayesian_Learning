---
title: '732A73: Bayesian Learning'
author: "Oriol Garrobé, Dávid Hrabovszki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  html_document: default
subtitle: Computer Lab 2
header-includes: \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, eval=TRUE)
```

```{r warning = FALSE, include=FALSE}
# R version
RNGversion('3.5.1')

# libraries
library(mvtnorm)
library(knitr)
```

# Question 1. Linear and polynomial regression.

Given the dataset \texttt{TempLinkoping.txt} containing:
* Temp: Response variable with daily average of temperatures in Linkoping over 2018.
* Time: Covariate of Temp computed as

$$
time=\frac{number\ of\ days\ since\ beginning\ of\ the\ year}{365}
$$

We are asked to perform Bayesian analysis of a quadratic regression

$$
temp=\beta_0 + \beta_1 \cdot time + \beta_2 \cdot time^2 + \epsilon, \epsilon \sim N(0,\sigma^2).
$$


### (a)
```{r}
############################################################
#####  Question 1. Linear and polynomial regression  #######
############################################################

### (a)

data <- read.table("TempLinkoping.txt", header = TRUE)

y <- data$temp
time <- data$time

n <- length(y)
X <- cbind(matrix(1, nrow = length(y)),time,time^2)

# Initial prior hyperparameters
mu_0 <- c(-10,100,-100)
omega_0 <- 0.1*diag(3)
nu_0 <- 4
sigma2_0 <- 1

# Conjugate prior
compute_betas <- function(N_draws, mu_0, omega_0, nu_0, sigma2_0){
  draw_sigma2 <- nu_0*sigma2_0/rchisq(Ndraws, nu_0)
  draw_beta <- matrix(0, nrow=length(draw_sigma2), ncol=3 )
  for (sigma in draw_sigma2) {
    i <- which(draw_sigma2==sigma)
    draw_beta[i,] <- rmvnorm(1, mu_0, sigma*omega_0)
  }
  return(colMeans(draw_beta))
}

Ndraws=1000

# Tune hyperparameters

# Initial hyperparameters
beta_0 <- compute_betas(Ndraws, mu_0, omega_0, nu_0, sigma2_0)

# 1st iteration
mu_1 <- c(-5,90,-90)
beta_1 <- compute_betas(Ndraws, mu_1, omega_0, nu_0, sigma2_0)

# 2nd iteration
mu_2 <- c(0,100,-100)
beta_2 <- compute_betas(Ndraws, mu_2, omega_0, nu_0, sigma2_0)

# 3rd iteration
mu_3 <- c(0,85,-85)
beta_3 <- compute_betas(Ndraws, mu_3, omega_0, nu_0, sigma2_0)

# 4th iteration
mu_4 <- c(-5, 85, -85)
beta_4 <- compute_betas(Ndraws, mu_4, omega_0, nu_0, sigma2_0)

mu_0 <- mu_1
```


```{r}
plot(time, X %*% beta_0, type = 'l', lwd=2, col='red', ylim=c(-10, 25),
     ylab='Temperature', xlab='Time')
points(time, X %*% beta_1, lwd=2, col='blue', type='l')
points(time, X %*% beta_2, lwd=2, col='pink', type='l')
points(time, X %*% beta_3, lwd=2, col='green', type='l')
points(time, X %*% beta_4, lwd=2, col='yellow', type='l')
```

### (b)

```{r}

### (b)

# Conjugate posterior

# Hyperparameters

beta_hat <- solve(t(X)%*%X)%*%t(X)%*%y
mu_n <- solve(t(X)%*%X + omega_0) %*% (t(X)%*%X%*%beta_hat + omega_0%*%mu_0)
omega_n <- t(X) %*% X + omega_0
nu_n <- nu_0 + n # what is n?????????????????????
nu_sigma2_n <- nu_0*sigma2_0+(t(y)%*%y + t(mu_0)%*%omega_0%*%mu_0 - t(mu_n)%*%omega_n%*%mu_n)
sigma2_n <- nu_sigma2_n[1] / nu_n

draw_sigma2 <- nu_n*sigma2_n/rchisq(Ndraws, nu_n)
draw_beta <- matrix(0, nrow=length(draw_sigma2), ncol=3 )
for (sigma in draw_sigma2) {
  i <- which(draw_sigma2==sigma)
  draw_beta[i,] <- rmvnorm(1, mu_n, sigma*omega_n)
}
```

```{r}

# Sigma
hist(draw_sigma2, freq = F, breaks=20, main="Sigma", col = 'grey', xlab='Sigma values')
```

```{r}

#Beta 1
hist(draw_beta[,1], freq = F, breaks=20, main="Beta 1", col="grey", xlab="Beta values")
```

```{r}

#Beta 2
hist(draw_beta[,2], freq = F, breaks=20, main="Beta 2", col="grey", xlab="Beta values")
```

```{r}

#Beta 3
hist(draw_beta[,3], freq=F, breaks=20, main="Beta 3", col="grey", xlab="Beta values")
```

```{r}
posterior_beta <- colMeans(draw_beta)
plot(time, y)
points(time, X %*% posterior_beta, lwd=2, col='yellow', type='l')
```


### (c)

```{r}
x_tilda <- -posterior_beta[2]/(2*posterior_beta[3]) #0.5453696
```

### (d)

```{r}

```



\pagebreak

# Question 2. Posterior approximation for classification with logistic regression

The dataset WomenWork.dat contains n = 200 observations (i.e. women) on the following nine variables:

Constant, HusbandInc, EducYears, ExpYears, ExpYears2, Age, NSmallChild, NBigChild

### a

We consider the logistic regression

$Pr(y=1|x)=\frac{exp(x^T\beta)}{1+exp(x^T\beta)}$

where $y=0$ is the woman does not work and $y=1$ if she does. $X$ is an 8-dimensional vector with the features including the intercept. In this part, we approximate the posterior distribution of the parameter vector $\beta$:

$\beta|y,X\sim N \Big(\tilde{\beta}, J_y^{-1}(\tilde{\beta})\Big)$

$\tilde{\beta}$ is the posterior mode and $J(\tilde{\beta}) = -\frac{\partial^2lnp(\beta|y)}{\partial\beta\partial\beta^T}|_{\beta=\tilde{\beta}}$ is the observed Hessian evaluated at the posterior mode. We use the following prior:
$\beta\sim N(0,\tau^2I)$, with $\tau = 10$.

The code for the approximation can be found and in the Appendix and was written using Mattias Villani's implementation as a template:
https://github.com/mattiasvillani/BayesLearnCourse/raw/master/Code/MainOptimizeSpam.zip

```{r}
# 2. Posterior approximation for classification with logistic regression

# 2.a

# The following code was written using Mattias Villani's implementation as a template:
# https://github.com/mattiasvillani/BayesLearnCourse/raw/master/Code/MainOptimizeSpam.zip

# read data
women = read.table('WomenWork.dat', header = T)

y = as.vector(women[,1])
x = as.matrix(women[,2:9])
tau = 10
nfeatures = dim(x)[2]

# prior
mu = as.vector(rep(0,nfeatures))
sigma = diag(tau^2, nfeatures, nfeatures)

LogPrior = function(Beta, mu, sigma){
  return(dmvnorm(Beta, as.matrix(mu), sigma, log = T))
}

# log likelihood
LogLikelihood = function(Beta, y, x){
  sum(y * x%*%Beta - log(1 + exp(x%*%Beta)))
}

# log posterior
LogPosterior = function(Beta, y, x, mu, sigma){
  LogPrior(Beta, mu, sigma) + LogLikelihood(Beta, y, x) 
  # we can sum them instead of multiplying, because of the log
}

# initialize Beta vector randomly
# Seed
set.seed(1234567890)
Beta_init = as.vector(rnorm(dim(x)[2]))

# optimizing the log posterior by changing the Betas (maximize)
res = optim(Beta_init, LogPosterior, gr = NULL, y, x, mu, sigma, 
                    method="BFGS", control=list(fnscale=-1), hessian=T)

Beta_hat = res$par # posterior mode
Hessian = res$hessian
post_sigma = solve(-Hessian) # posterior cov matrix
stdev = sqrt(diag(post_sigma))

# approximate 95% credible interval for NSmallChild
lower = Beta_hat[7] - 1.96 * stdev[7] #-2.121445
upper = Beta_hat[7] + 1.96 * stdev[7] #-0.5968567

```


We obtained the following posterior mode for $\beta$:

```{r results='asis'}
betas = t(as.matrix(Beta_hat))
colnames(betas) = c("Beta1","Beta2","Beta3","Beta4","Beta5","Beta6","Beta7","Beta8")
kable(betas)
```

And the posterior covariance matrix $J_y^{-1}(\tilde{\beta})$:

```{r results='asis'}
sigma_table = data.frame(post_sigma)
colnames(sigma_table) = c("Beta1","Beta2","Beta3","Beta4","Beta5","Beta6","Beta7","Beta8")
rownames(sigma_table) = c("Beta1","Beta2","Beta3","Beta4","Beta5","Beta6","Beta7","Beta8")
kable(round(sigma_table,5))

```

We also compute an approximate 95% credible interval for the coefficient of the NSmallChild variable: $[-2.121445, -0.5968567]$
NSmallChild is an important determinant of whether a woman is working or not, because it has the highest absolute valued posterior coefficient $\tilde{\beta}$ of all features. The posterior mode of its coefficient is -1.36, which means that the more small children a woman has, the less likely it is that she's working, because the sign is negative.

To verify that we got the correct result, we compare it to maximum likelihood estimates, and note that the numbers are very similar.

```{r}
# check results
glmModel <- glm(Work~0 + ., data = women, family = binomial)
kable(t(as.matrix(glmModel$coefficients))) #roughly the same as Beta_hat
```



### b

In this part we simulate from the predictive distribution of the response variable in a logistic regression. More specificially we simulate and plot the predictive distribution for the Work variable for a 40 year old woman, with two children (3 and 9 years old), 8 years of education, 10 years of experience. and a husband with an income of 10. To do this, first we draw random numbers from the multivariate normal distribution, with mean $\tilde{\beta}$ and covariance matrix $J_y^{-1}(\tilde{\beta})$. Since in normal distributions the mean is equal
to the mode, we can use the result from 2.a. Then we apply our implementation of the logistric regression function defined above to the given woman with every random draw. 

```{r}
# 2.b
# The mode of a normal distribution is equal to the mean.

log_reg = function(Beta, x){
  return(exp(x%*%Beta) / (1 + exp(x%*%Beta)))
}
Constant = 1
HusbandInc = 10
EducYears = 8
ExpYears = 10
ExpYears2 = (ExpYears/10)^2
Age = 40
NSmallChild = 1
NBigChild = 1

x_pred = c(Constant, HusbandInc, EducYears, ExpYears, ExpYears2, Age, NSmallChild, NBigChild)

# simulate posterior draws from Beta
n = 10000
post_sim = rmvnorm(n, mean = Beta_hat, sigma = post_sigma) # should i add pop variance to post_sigma?

# calculate target y
pred = apply(post_sim, 1, log_reg, x_pred)
```

The resulting distribution in Figue \ref{fig:hist_bernoulli} shows that the given woman is much more likely to not be working, because the mode is around 0.2. This is mostly because she has a small child, which as we've seen before, is correlated negatively with working.

```{r fig.cap="\\label{fig:hist_bernoulli} Histogram of the predictive distribution of the response variable of a woman with given features", out.width = "80%", fig.pos='!h', fig.align='center'}
hist(pred, freq = F, main = "", xlab = "")
lines(density(pred))
```


### c

Now, we consider 10 women which all have the same features as the woman in 2(b). The random variable of whether a woman is working or not follows a Bernoulli distribution, but if we add 10 such random variables, we get a Binomial distribution, with parameters n = 10 and p = p(given woman works). We calculate the probability of working as the mean of the Bernoulli random variables calculated in 2.b.

```{r}
# 2.c
# probability of success (working)
p = mean(pred)

binom_sim = rbinom(10000, 10, p)
```


```{r fig.cap="\\label{fig:hist_binom} Histogram of the predictive distribution of the number of women working (out of 10)", out.width = "80%", fig.pos='!h', fig.align='center'}
hist(binom_sim, freq = F, main = "", xlab = "")
```
Figue \ref{fig:hist_binom} shows that 2 such women working has the highest probability. 

\pagebreak

# Appendix

```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```



